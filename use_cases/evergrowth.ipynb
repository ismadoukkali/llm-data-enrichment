{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data enrichment with LLM's | V1 Evergrowth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"/Users/ismadoukkali/Desktop/llm-data-enrichment/diagrams/evergrowth_diagrams.png\" alt=\"Alt text\" title=\"Image Title\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.request import urlopen, Request\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.llms import OpenAI\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from zenrows import ZenRowsClient\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI API functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_response_gpt4(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant, expert in Analysing Companies.\"},\n",
    "        {\"role\": \"user\", \"content\": str(prompt)},\n",
    "    ],\n",
    "    max_tokens=300,\n",
    "    temperature=0\n",
    "    )\n",
    "    selection = response.choices[0].message.content\n",
    "    return selection\n",
    "\n",
    "def generate_response_gpt4_json(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    response_format={ \"type\": \"json_object\" },\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant, expert in Analysing Companies.\"},\n",
    "        {\"role\": \"user\", \"content\": str(prompt)},\n",
    "    ],\n",
    "    temperature=0\n",
    "    )\n",
    "    selection = response.choices[0].message.content\n",
    "    return selection\n",
    "\n",
    "def generate_response_gpt3_json(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    response_format={ \"type\": \"json_object\" },\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant, expert in Analysing Companies.\"},\n",
    "        {\"role\": \"user\", \"content\": str(prompt)},\n",
    "    ],\n",
    "    max_tokens=300,\n",
    "    temperature=0\n",
    "    )\n",
    "    selection = response.choices[0].message.content\n",
    "    return selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt to identify relevant links within sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_identify_links = \"\"\"\n",
    "\n",
    "Identify whether this url handle is relevant to identify whether this company holds physical stores of their own and/or have partnership with retailers. \n",
    "\n",
    "This includes details about the e-commerce platform, physical store locations, wholesale operations, and partnerships with external retailers.\n",
    "\n",
    "Common URL Handles:\n",
    "\n",
    "/about-us or /company-info: Often contains comprehensive information about the company's history, business model, and operational strategies.\n",
    "/wholesale or /b2b: Pages specifically dedicated to wholesale operations and business-to-business sales.\n",
    "/store-locator or /find-a-store: Helps in identifying physical store locations, indicating a brick-and-mortar sales channel.\n",
    "/our-partners, /retailers or /dealers: Information about partnerships with external retailers.\n",
    "/investor-relations or /press: Might contain detailed information about business operations for stakeholders.\n",
    "\n",
    "\n",
    "If the link is relevant, output 'yes' if not, output 'no'. Output the result in JSON format:\n",
    "\n",
    "{\n",
    "    \"link\": \"input url\"\n",
    "    \"relevant\": \"yes\" or \"no\"\n",
    "}\n",
    "\n",
    "Here the site: \n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt to craft description for what the business does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organize_prompt = '''\n",
    "From the following scrapped text of a website, explain what the business sells exactly, the business model and the consumer channels this company utilises to sell. \n",
    "\n",
    "The target consumer channels to look for are: \n",
    "1. Ecommerce\n",
    "2. Owned Physical Stores\n",
    "3. Wholesale / Retail Stores and/or Marketplaces\n",
    "\n",
    "In bulletpoints, indicate if this company operates in the consumer channels above.\n",
    "\n",
    "This company description is going to be fed to a company categorization model where the combination of the channels for selling like own physical stores and/or traditional retail channels and/or ecommerce websites is a crucial component of the classification.\n",
    "\n",
    "Search for keywords and indicators that can inform all of the variables mentioned above like button names and else... if in doubt for a selling channel, be determinisic and always include it if its mentioned. Don't doubt mentions in the context.\n",
    "\n",
    "Write everything in third person naming the company. Utilise terminology that is accurate for the specific industry and business model. \n",
    "\n",
    "Make this analysis short and concise.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt to classify into specific category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_prompt = \"\"\"\n",
    "                    \n",
    "    Classify the following described company within the provided categories, taking into account its what it sells, its business model, terminology used to described it and\n",
    "    the sales channels the company uses to operate. \n",
    "    \n",
    "    Stay within the categories mentioned in the context, never deviate. These categories are:\n",
    "    \n",
    "    \"Pure-player D2C Brand\": \"A Pure-player D2C (Direct-to-Consumer) Brand specializes in selling products online directly to consumers, bypassing traditional retail or physical store channels. \n",
    "    Pure-player D2C Brands sell their goods directly to consumers via their website with no intermediaries, retailers or wholesalers.\n",
    "\n",
    "    \"Omnichannel D2C Brand\": \"An Omnichannel D2C Brand combines their own physical store presence with a robust online platform to sell products directly to their consumers.\n",
    "    These brands sell exclusively through their own physical locations and their online channels, by passing traditional retailers. They are known for their global reach, trendsetting products, and innovative retail strategies. \n",
    "\n",
    "    \"Omnichannel Retailer\": \"Omnichannel Retailers offer a unified traditional retail shopping experience through their network of physical stores and digital platforms selling products from other brands. \n",
    "    They prioritize customer convenience, integrating online and offline channels for a seamless transaction process. \n",
    "\n",
    "    \"Pure-player Retailer\": \"Pure-player Retailers operate exclusively through e-commerce platforms focusing on the resale of branded products that are not of their own. \n",
    "    These retailers do not engage in product manufacturing, emphasizing online sales, logistics, and digital customer service.\n",
    "\n",
    "    \"Retail Branded Goods Manufacturer\": \"Retail Branded Goods Manufacturers design, produce, and sell their products, leveraging e-commerce, potentially their own physical stores and conventional retail channels/wholesalers. \n",
    "\n",
    "    \"FMCG & CPG\": \"FMCG (Fast-Moving Consumer Goods) & CPG (Consumer Packaged Goods) companies specialize in manufacturing, designing and producing of perishable goods which are sold exclusively through traditional retail channels. \n",
    "    All companies that focus on low-cost products, often used daily, like groceries, foods, cosmetics, drinks and household items should be classified in this category and that have as main channels of selling wholesale retailers. \n",
    "    They might have some activity in ecommerce but it may be minimal.\n",
    "\n",
    "    \"Online Marketplace\": \"Online Marketplaces provide a digital platform for third-party sellers and buyers to transact. They facilitate a wide range of goods, from consumer electronics to handmade crafts. These platforms focus on e-commerce efficiency, digital transactions, and customer-seller interaction without engaging in product manufacturing or branding. The Online Marketplace does not own the stock of goods sold on its platform, \n",
    "    it only provides the space for professional sellers and buyers to interact.\n",
    "\n",
    "    \"Classified Ads\": \"Companies in the Classified Ads sector facilitate the sale of goods through advertisement platforms. They provide a space for individual sellers and individual buyers to connect, often for second-hand or niche items, without directly engaging in the sales process themselves. Compared to online marketplaces, where the sellers tend to be established businesses, classified ad companies give a peer-to-peer space where stakeholders can sell their items.\n",
    "    Here some additional keywords related to the type of company: classified advertising, second-hand goods platform, buyer-seller advertisement, online classifieds, print classified ads, niche product sales, individual seller platform, advertisement-based sales, peer-to-peer marketplace, local goods advertising.\"\n",
    "\n",
    "    \"ICP no\": \"Companies which do not fall within the e-commerce and/or retail business model are classified as ICP no. These include companies with models similar to like SaaS, Apps or others different from e-commerce or retail.\n",
    "                                \n",
    "    Here the description of the company in question:\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for google search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_search(query):\n",
    "    API_KEY = os.getenv(\"GOOGLE_SEARCH_API\")\n",
    "    SEARCH_ENGINE_ID = '3232eee26d51543f1'\n",
    "\n",
    "    url = 'https://www.googleapis.com/customsearch/v1'\n",
    "\n",
    "    params = {\n",
    "        'key': API_KEY,\n",
    "        'cx': SEARCH_ENGINE_ID,\n",
    "        'q': query\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        if 'items' in results:\n",
    "            return results['items'][0]['link']\n",
    "        else:\n",
    "            return 'No results found'\n",
    "    else:\n",
    "        return f'Error: {response.status_code}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_string(input_string):\n",
    "    if len(input_string) <= 3000:\n",
    "        return input_string\n",
    "    else:\n",
    "        return input_string[:3000]\n",
    "\n",
    "def format_url(url):\n",
    "    if url.startswith(\"http://www.\"):\n",
    "        url = url.replace(\"http://www.\", \"https://www.\")\n",
    "    elif url.startswith(\"http://\"):\n",
    "        url = url.replace(\"http://\", \"https://www.\")\n",
    "    elif url.startswith(\"www.\"):\n",
    "        url = \"https://\" + url\n",
    "    elif not url.startswith(\"https://\"):\n",
    "        url = \"https://www.\" + url\n",
    "    return url\n",
    "\n",
    "def retrieve_html_old(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    req = Request(url, headers=headers)\n",
    "    html = urlopen(req).read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract() \n",
    "\n",
    "    text = soup.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zenrow API function scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_html(url):\n",
    "    client = ZenRowsClient(os.getenv(\"ZEN_ROWS_SCRAPER\"))\n",
    "\n",
    "    params = {\"block_resources\": \"image,media,font\",\n",
    "              \"premium_proxy\":\"true\"}\n",
    "\n",
    "    response = client.get(url, params=params)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Process the HTML as before\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "\n",
    "    text = soup.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to extract all links from sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_links(base_url):\n",
    "    client = ZenRowsClient(os.getenv(\"ZEN_ROWS_SCRAPER\"))\n",
    "\n",
    "    params = {\"block_resources\": \"image,media,font\",\n",
    "            \"premium_proxy\":\"true\"  }\n",
    "\n",
    "    response = client.get(base_url, params=params)\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        a_tags = soup.find_all('a')\n",
    "        links = set()  \n",
    "        links.add(base_url)\n",
    "        for tag in a_tags:\n",
    "            if 'href' in tag.attrs:\n",
    "                full_url = urljoin(base_url, tag['href'])\n",
    "                if urlparse(full_url).netloc == urlparse(base_url).netloc:\n",
    "                    links.add(full_url)\n",
    "    \n",
    "        return links\n",
    "    except Exception:\n",
    "        print(\"Failed to retrieve the main webpage.\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "#url = \"https://wholeearthsweetener.com/\"  # Replace with the desired URL\n",
    "#links = extract_all_links(url)\n",
    "#print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "executor = ThreadPoolExecutor(100)\n",
    "\n",
    "def extract_relevant_links(base_link, links):\n",
    "    website = prompt_identify_links\n",
    "    futures = []\n",
    "    for link in links:\n",
    "        future = executor.submit(generate_response_gpt3_json, (website + '' + link))\n",
    "        futures.append(future)\n",
    "\n",
    "    relevant_links = set()\n",
    "    for future in futures:\n",
    "        result_json = json.loads(future.result())\n",
    "        answer = result_json[\"relevant\"]\n",
    "        relevant_link = result_json[\"link\"]\n",
    "        if answer == \"yes\":\n",
    "            relevant_links.add(relevant_link)\n",
    "    \n",
    "    relevant_links.add(base_link)\n",
    "    print('Went from all links in site: ', len(links))\n",
    "    print('To this size: ', len(relevant_links))\n",
    "    print('Relevant links: ', relevant_links)\n",
    "\n",
    "    return relevant_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to build context file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sites(urls):\n",
    "    all_text = \"\"\n",
    "    for url in urls:\n",
    "        try:\n",
    "            text = retrieve_html(url)\n",
    "            all_text += text + \"\\n\\n\" + \"------------------------------------------------\" + \"\\n\\n\" \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving {url}: {e}\")\n",
    "\n",
    "    with open('data/site_data.txt', 'w'):\n",
    "        pass\n",
    "    \n",
    "    with open('data/site_data.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(all_text)\n",
    "    \n",
    "    print('Company context succesfully updated :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main loop function with timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "import time\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "def run_function_with_timeout(industryGPT, company):\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    \n",
    "    signal.alarm(200)  \n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        result = industryGPT(company)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"industryGPT: Completed in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        signal.alarm(0)\n",
    "        \n",
    "        return result\n",
    "    except TimeoutException:\n",
    "        print('Result took too long to output.')\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "signal.alarm(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "def industryGPT(company):\n",
    "    try:\n",
    "        all_links = extract_all_links(company)\n",
    "        relevant_links = extract_relevant_links(company, all_links)\n",
    "        scrape_sites(relevant_links)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print('Error when looking at site, ', company, ':', e)\n",
    "        return None\n",
    "\n",
    "    from llama_index.llms import OpenAI\n",
    "    from llama_index import ServiceContext\n",
    "    \n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    llm = OpenAI(model=\"gpt-4-1106-preview\", temperature=0, max_tokens=300)\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents, service_context=service_context\n",
    "    )\n",
    "\n",
    "    query_engine = index.as_query_engine(\n",
    "        response_mode=\"refine\", streaming=True, similarity_top_k=1\n",
    "    )\n",
    "    \n",
    "\n",
    "    description = query_engine.query(organize_prompt)\n",
    "    \n",
    "    print()\n",
    "    # description.print_response_stream()\n",
    "\n",
    "    description_no_streaming = \"\"\n",
    "\n",
    "    for text in description.response_gen:\n",
    "        print(text, end='')\n",
    "        description_no_streaming += text\n",
    "        pass\n",
    "\n",
    "    # print(description_no_streaming)\n",
    "    \n",
    "\n",
    "    classification = generate_response_gpt4_json(categories_prompt + description_no_streaming + \"\"\"\n",
    "\n",
    "                            Output your response in json format like this: \n",
    "                            \n",
    "                            {\n",
    "                                name: \"company name\",\n",
    "                                category: \"identified category\"\n",
    "                            }\n",
    "\n",
    "                            Never deviate from the categories above, always stay within the taxonomy. \n",
    "                            If the category is not possibly identified just output \"ICP no\" there.\n",
    "\n",
    "                            \"\"\")\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    response_dict = json.loads(classification)\n",
    "    print(\"Company name: \", response_dict[\"name\"])\n",
    "    print(\"Category: \", response_dict[\"category\"])\n",
    "    print(\"JSON: \", response_dict)\n",
    "    response_dict[\"description\"] = description_no_streaming\n",
    "    print()\n",
    "    print('--------------------')\n",
    "    print()\n",
    "\n",
    "\n",
    "    return response_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One enrichment demo\n",
    "\n",
    "Run from here to get test enrichment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = run_function_with_timeout(industryGPT, \"https://wholeearthsweetener.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple enrichment demo's\n",
    "Run from here for a batch enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"Name\", \"Category\", \"Description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_of_companies = [\"https://wholeearthsweetener.com/\", \"https://www.charlottesweb.com/\", \"https://www.factor75.com/\", \n",
    "                      \"https://candyhackers.com/\", \"https://us.air-up.com/\", \"https://www.arrae.com/\", \"https://www.greenchef.com/\", \n",
    "                      \"https://thevitacococompany.com/\", \"https://www.gumtreegolfandnature.com/\", \"http://www.epicbar.com\", \n",
    "                      \"https://blendjet.com/\", \"https://chomps.com/\", \"https://effingoodsnacks.com/\", \"https://joinfightcamp.com/\", \n",
    "                      \"https://jackweirandsons.com/\", \"http://www.thepurplecarrot.com\", \"https://www.maximustribe.com/\", \"https://www.fuelmeals.com/\", \n",
    "                      \"https://gfuel.com/\", \"https://www.freshnlean.com/\", \"https://havenskitchen.com/\", \"https://www.hungryroot.com/\", \"https://www.justmeats.com/\", \n",
    "                      \"https://www.gumtreegolfandnature.com/\", \"https://www.cookunity.com/\", \"https://jain.golf/\", \"https://www.oatsovernight.com/\", \n",
    "                      \"https://www.nutpods.com/\", \"https://www.myollie.com/\", \"https://opalcamera.com/\", \"https://poponveneers.com/\", \"https://publicdrip.com/\", \n",
    "                      \"https://www.readyrefresh.com/\", \"https://softframedesigns.com/\", \"https://mymetabolicmeals.com/\", \"https://sprinly.com/\", \"https://smartfinancial.com\", \n",
    "                      \"https://www.suvie.com/\", \"https://www.tempomeals.com/\", \"https://aliceandolivia.com\", \"https://www.trifectanutrition.com/\", \"https://fender.com\"]\n",
    "\n",
    "enrichment = []\n",
    "for company in array_of_companies:  \n",
    "        \n",
    "    name = None   \n",
    "    category = None \n",
    "    \n",
    "    print()\n",
    "    try:\n",
    "        print('Retrieving: ', format_url(str(company)))\n",
    "        response = run_function_with_timeout(industryGPT, company)\n",
    "        name = response[\"name\"]\n",
    "        category = response[\"category\"]\n",
    "        description = response[\"description\"]\n",
    "\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error while enriching site: \", company)\n",
    "        print()\n",
    "        pass\n",
    "\n",
    "    print('--------------------------------')\n",
    "\n",
    "    new_row = { \"Name\": name,\n",
    "                \"Website\" : company,\n",
    "                \"Category\": category,\n",
    "                \"Description\": description\n",
    "            }\n",
    "    \n",
    "    df.loc[len(df)] = new_row\n",
    "\n",
    "df.to_csv(\"Evergrowth_Enriched.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
